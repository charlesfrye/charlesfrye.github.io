---
layout: post
title:	"Taking 'LLMs Are The New Kernels' Seriously - Research Roundup"
date:	2023-11-01
category: programming
---

![karpathy-llm-kernel-tweet]
{: style="text-align: center"}

Andrej Karpathy is not the only one to connect
contemporary large language models (LLMs)
with early processors, kernels, and operating systems.

In fact, there's something of a wave of research at this intersection,
ranging from novel approaches to pretraining
to inference-time speed optimizations
to prompting strategies that make LLMs act more like kernels.

This post is a collection of short explainers for papers
that directly draw from systems metaphors
in designing improvements for LLMs.
It is intended to get researchers and hackers of LLMs
interested in systems
and vice versa.
<!--exc-->

## Speculative Execution

### Speculative Execution in Processors

In speculative execution,
bottlenecks are mitigated by executing
the work that _most likely_ follows them.

One example of speculative execution is branch prediction.

In branch prediction, rather than waiting for the result of a conditional
to be available in order to determine which branch to execute,
the processor simply proceeds with the "most likely" branch,
based on compile-time hints or runtime statistics.

This can lead to large speedups when a particular branch
of a conditional is taken only rarely
(e.g. unhappy error handling paths)
and requires a lot of work to execute
(e.g. subject to cache misses).

Speculative execution is infamous
thanks to the
[Spectre](https://arxiv.org/abs/1801.01203)
vulnerability,
which manipulates runtime statistics to
"trick" the processor into executing
out-of-bounds memory reads.

Undaunted by that cautionary tale of a
[multi-billion-dollar vulnerability](https://www.statista.com/statistics/800258/worldwide-meltdown-spectre-potential-mitigation-cost-by-device-type/),
let's consider how we might replicate
this pattern of
executin cheap but possibly wrong operations
to speed up LLM inference.

### [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318) and [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)

Two roughly-simultaneous papers from frenemies
Google Research and DeepMind
demonstrated similar methods for large speedups in LM generation
by using cheaper models to predict certain tokens
without changing the system's output probabilities --
speculatively generating a number of draft tokens
before the LM is used to compute the final outputs.

The intuition behind speculative sampling is that
some of the tokens in the output are very easy to guess --
a period will come at the end of this sentence, for example --
and so can be predicted very cheaply.

These methods further take advantage of the fact that
computing the logprobs for
a prompt + K tokens can be done in parallel
and so is much cheaper than sampling K tokens to follow a prompt,
which must be done serially --
a painful and unavoidable fact of autoregressive models like Transformers or RNNs.

The K tokens following the prompt are generated by some cheap strategy --
a smaller language model or even a heuristic rule.
We then compute the logprobs for the prompt + K tokens
in the large model, in parallel,
and then use a well-known trick going back to the atomic age,
rejection sampling, to throw out only those tokens that are improbable
according to the large model --
in direct proportion to how improbable they are.

For temperature 0, aka deterministic sampling,
this equates to throwing out all tokens that follow
the first wrong token put out by the speculative process.

The basic flow is captured well
in this animation of speculative sampling from
[an excellent explainer by Joao Gante of Hugging Face](https://huggingface.co/blog/assisted-generation):

<video style="width: 90%" autoplay="" loop="" muted="" playsinline="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_4_1080p.mov"></video>

As with speculative decoding, we do some work that might be wasted:
running the draft/speculative model
and scoring logprobs on all K tokens.

But because running the model on the prompt + K tokens is roughly the same cost
as executing on the prompt + 1 token, for small K,
we almost always come out ahead --
just as speculative execution comes out ahead because
reading from memory and then executing the right branch
is roughly the same cost as executing a wrong branch while reading from memory and then executing the right branch.

When used in a hierarchical configuration --
regexes speculating for BERTS that speculate for GPTs --
speculative sampling can achieve
2x speedups or greater.

Note that 2-3x is the typical cost difference between
prompt tokens and output tokens in LLM APIs,
and so well-executed speculative decoding
can reduce that cost difference to almost nothing
(thanks to the team at
[Replicate](https://replicate.ai/)
for thoughtful discussions on this).


## Registers

### Registers in Processors

Registers store small amounts of intermediate data
during computations.

Unlike stack-allocated or heap-allocated memory,
these values are stored in the processor itself
and don't have semantic meaning
in a high-level language like C.

How might this pattern show up in LLMs?

### [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588)

Alright, this is cheating a bit, because it's Vision Transformers,
rather than Transformer language models,
but it's still a cool paper so I'm going to explain it
(there's also a weaker but similar result for LLMs,
based on
["pause tokens"](https://arxiv.org/abs/2310.02226)).

This paper claims that
1. Vision Transformers use uninformative pixels
as dynamic registers for storing intermediate information
2. You can intentionally add empty "register tokens" to the input to remove this behavior

The first point is backed up by this gorgeous figure:

![vision-transformers-need-registers]
{: style="text-align: center"}

The bright colors indicate areas of the image to which the models attended
when producing their predictions.
Notably, large ViTs are attend heavily to
uninformative, background pixels.

Does this mean the models are learning to use artifacts of the dataset
or side channel information to make their predictions?

Not necessarily!
Transformers are a highly residual architecture,
and one of the most popular ways to understand how they work,
courtesy of
[work from Anthropic's interpretability team](https://transformer-circuits.pub)
is in terms of a "residual stream".

In this view, it is emphasized that each layer interacts additively
with its inputs, allowing Transformer layers to treat their inputs
like a content-addressable memory,
reading information written by earlier layers and writing in some of their own.

It's a neat way to do computation,
but it has a big issue:
if you want to write information that's not tied to a specific token position,
you don't have a good place to put it.

It is hypothesized in the paper that large ViTs learn to use
the least-informative token areas as a kind of scratchpad --
like stealing a block of heap memory,
using it to store intermediate values,
and hoping you don't break anything.

The paper proposes adding some blank "register" tokens
to the input in order to give the model a safe spot to store
the results of calculations that don't otherwise have a place to go.

The empiricial results in terms of benchmarks are not incredible,
but the resulting attention maps are much cleaner,
as is obvious from the figure above,
and so one might expect some downstream benefits,
e.g. to interpretability or steerability.

## Paged Memory

### Paged Memory in Processors

Memory must be allocated, and those allocations must be tracked and managed,
e.g. defragmented, to maximize utilization of memory resources,
which were precious even before the Browser Nation attacked.

Memory allocators must handle a gnar trade-off:
address-wise allocations make efficient use of memory in the best case,
but lead to fragmentation and expensive allocator operations in reality,
while large block allocations are easy to manage but can lead to
wasted memory if the allocations are not fully utilized --
aka "internal fragmentation".

The solution is to split the difference and allocate memory in
multiples of moderately-sized blocks called "pages".

These pages can be assigned to processes,
loaded into caches,
and otherwise managed as bulk units,
while still being small enough to be efficiently utilized.

How might we apply this pattern of memory management to LLMs?

### [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)

The focus of memory management in LLM inference is on the
key-value ("KV") cache,
used to store the intermediate values of attention computations,
for re-use when computing the outputs for future tokens in the sequence.
This cache converts a quadratic-time operation into a linear-time one,
at a linear cost in space.

(If hot-swappable LoRA adapters ever take off,
a similar amount of memory ledgerdemain will be required
for the weights as well.)

The KV cache is typically allocated as a single large block of memory,
which is subject to the same issues as large block allocations in general.

This problem is especially acute for batched requests to LLMs,
which due to varying prompt lengths and varying response lengths,
can lead to very uneven utilization of the KV cache.

The situation is depicted nicely in this figure from the paper
that introduced PagedAttention and the vLLM inference server
that uses it.
Note that the "internal fragmentation" sections
should be several hundred times larger than they are depicted here!

![paged-attention-frag]
{: style="text-align: center"}

The solution proposed by the paper is to allocate the KV cache
via pages -- each one with enough space to hold a handful of states --
and use a page table, much like the one in a processor,
to map the logical memory of the cache
to its physical address.

Like this:

![paged-attention-table]
{: style="text-align: center"}

The vLLM inference server that uses this technique
is much faster than peer systems
and allocates far less memory to serve the same workloads.

## Virtual Memory

### Virtual Memory in Processors

In the beforetimes,
programmers stored information at addresses with names like
[`0x0BFF`](https://www.urbandictionary.com/define.php?term=bffaeae),
and they really meant it.

They could expect that if they took their trusty oscilloscope to the 3,071st
memory cell of their spacious 64KB of RAM,
they would find humming electrons vibing to the pattern they had stored.

This has an obvious problem:
if two 00s tween programmers attempt to use the one and only `0x0BFF` address
to store program data at the same time,
someone's data will be overwritten or read improperly --
akin to the problems of addressing more than one person as your BFF.

The solution is to use a virtual memory system:
programmers and their programs are presented with a gorgeous row
of linear addresses,
from `0x00000000` to `0xFFFFFFFF`
(or whatever the highest address is),
and the operating system and hardware map those addresses to physical memory.

This indirection is also useful for pages!
Instead of mapping individual addresses,
virtual memory maps typically operate at the page level.

Virtual memory's indirection allows a process to address much more
than the available RAM --
for example virtual memory addresses can be mapped to locations on disk,
or on somebody else's computer, like one owned by Jeff Bezos.

RAM is limited and expensive, relative to disk,
so being able to use disk as memory is a big win.

Language models also have memory limits:
when producing tokens,
they can only refer to at most a fixed number of previous context tokens.

Hard limits on token count have rapidly increased from the few thousands
to the few hundred thousands.
But
[models struggle to effectively make use of those long contexts](https://arxiv.org/abs/2307.03172).

How might we apply the pattern of virtual memory to LLMs
to also allow them to effectively access much larger storage?

### [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)

The MemGPT pattern uses prompting and careful design of tools
to give LLMs the ability to manage their context --
treating the context as the physical memory
and all of the rest of the addressable information as virtual memory.

The pattern is depicted in this figure from the paper:

![memgpt-system-diagram]
{: style="text-align: center"}

As in retrieval-augmented generation (RAG) patterns,
the addressable information is stored outside the LLM
and interfaced with by traditional software.
That means it can be a database, a filesystem, or any other form of storage.

But unlike typical RAG,
retrieval from the system is done by the LLM,
via function-calling tool use,
rather than being hard-coded.
This style is associated more with agent patterns
and is
[used in the OpenAI Assistants API](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval).

The LLM is prompted to retrieve the information it needs and add it to its prompt --
much like virtual memory stored on disk can be paged into RAM.

Even further down the path to agency and away from simple RAG,
in the MemGPT pattern the LLM is also responsible for
writing information that overflows the context window
back to the storage system.
This pattern was popularized by the
[Generative Agents paper](https://arxiv.org/abs/2304.03442),
which uses LLMs to drive video game NPCs.
There, agents had the ability to "reflect" on their experiences
and write those reflections into their memory.

A key novelty of this paper,
relative to these other similar systems,
is the event-driven style:
the LLM sits idle,
waiting for events --
like user messages,
timer ticks,
or state changes in the world.

This is an extremely common pattern in the systems world --
on every keystroke, button press, or filesystem change,
a typical operating system interrupts running processes
to respond to the event,
e.g. to pass the keystroke to the process managing the active window.

Interruptibility and event-driven-ness are key features of biological agents as well.
To quote Copilot's suggested completion of this paragraph:
"If you poke a cat, it will stop what it's doing and respond to the poke."

The event of that suggestion's appearance while drafting this post
interrupted my drafting task, triggering me to reflect on it
and then resume my drafting task with a new idea for what to say next.

## What Next?

It is clear that large language models --
and large generative models of complex data types more generally --
are a fundamental, foundational building block,
like relational data stores or operating system kernels.

ML researchers like me have many useful intutions about this novel building block,
but the builders of those past blocks have many more,
and many that are orthogonal or surprising to someone who has spent more time
with Bayesian networks than with the network stack.

And as is perhaps obvious to systems folks from the vintage of many of the ideas above --
virtual memory was invented in the late 1950s
and mainstreamed in the 80s --
there is a lot of low-hanging fruit in applying systems patterns to LLMs.

So whether your background is in systems or ML,
I hope this post has inspired you to think
"across the divide" and apply your knowledge to the other field.

[karpathy-llm-kernel-tweet]: {{site.imgurl}}/karpathy-llm-kernel-tweet.png
[vision-transformers-need-registers]: {{site.imgurl}}/vision-transformers-need-registers.png
[paged-attention-frag]: {{site.imgurl}}/paged-attention-frag.png
[paged-attention-table]: {{site.imgurl}}/paged-attention-table.png
[memgpt-system-diagram]: {{site.imgurl}}/memgpt-system-diagram.png
